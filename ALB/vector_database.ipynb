{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b0c13be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\alvar\\documents\\dataavengers\\.venv\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\alvar\\documents\\dataavengers\\.venv\\lib\\site-packages (4.51.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\alvar\\documents\\dataavengers\\.venv\\lib\\site-packages (4.13.4)\n",
      "Requirement already satisfied: pdfplumber in c:\\users\\alvar\\documents\\dataavengers\\.venv\\lib\\site-packages (0.11.6)\n",
      "Requirement already satisfied: PyMuPDF in c:\\users\\alvar\\documents\\dataavengers\\.venv\\lib\\site-packages (1.25.5)\n",
      "Requirement already satisfied: langchain_core in c:\\users\\alvar\\documents\\dataavengers\\.venv\\lib\\site-packages (0.3.60)\n",
      "Requirement already satisfied: langchain_experimental in c:\\users\\alvar\\documents\\dataavengers\\.venv\\lib\\site-packages (0.3.4)\n",
      "Requirement already satisfied: langchain_community in c:\\users\\alvar\\documents\\dataavengers\\.venv\\lib\\site-packages (0.3.24)\n",
      "Requirement already satisfied: langchain_huggingface in c:\\users\\alvar\\documents\\dataavengers\\.venv\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\alvar\\documents\\dataavengers\\.venv\\lib\\site-packages (1.11.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\alvar\\documents\\dataavengers\\.venv\\lib\\site-packages (2.2.3)\n",
      "Collecting spacy\n",
      "  Using cached spacy-3.8.2.tar.gz (1.3 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  √ó pip subprocess to install build dependencies did not run successfully.\n",
      "  ‚îÇ exit code: 1\n",
      "  ‚ï∞‚îÄ> [65 lines of output]\n",
      "      Ignoring numpy: markers 'python_version < \"3.9\"' don't match your environment\n",
      "      Collecting setuptools\n",
      "        Using cached setuptools-80.7.1-py3-none-any.whl.metadata (6.6 kB)\n",
      "      Collecting cython<3.0,>=0.25\n",
      "        Using cached Cython-0.29.37-py2.py3-none-any.whl.metadata (3.1 kB)\n",
      "      Collecting cymem<2.1.0,>=2.0.2\n",
      "        Using cached cymem-2.0.11-cp313-cp313-win_amd64.whl.metadata (8.8 kB)\n",
      "      Collecting preshed<3.1.0,>=3.0.2\n",
      "        Using cached preshed-3.0.9.tar.gz (14 kB)\n",
      "        Installing build dependencies: started\n",
      "        Installing build dependencies: finished with status 'done'\n",
      "        Getting requirements to build wheel: started\n",
      "        Getting requirements to build wheel: finished with status 'done'\n",
      "        Preparing metadata (pyproject.toml): started\n",
      "        Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "      Collecting murmurhash<1.1.0,>=0.28.0\n",
      "        Using cached murmurhash-1.0.12-cp313-cp313-win_amd64.whl.metadata (2.2 kB)\n",
      "      Collecting thinc<8.4.0,>=8.3.0\n",
      "        Using cached thinc-8.3.6-cp313-cp313-win_amd64.whl.metadata (15 kB)\n",
      "      Collecting numpy<2.1.0,>=2.0.0\n",
      "        Using cached numpy-2.0.2.tar.gz (18.9 MB)\n",
      "        Installing build dependencies: started\n",
      "        Installing build dependencies: finished with status 'done'\n",
      "        Getting requirements to build wheel: started\n",
      "        Getting requirements to build wheel: finished with status 'done'\n",
      "        Installing backend dependencies: started\n",
      "        Installing backend dependencies: finished with status 'done'\n",
      "        Preparing metadata (pyproject.toml): started\n",
      "        Preparing metadata (pyproject.toml): finished with status 'error'\n",
      "        error: subprocess-exited-with-error\n",
      "      \n",
      "        √É‚Äî Preparing metadata (pyproject.toml) did not run successfully.\n",
      "        √¢‚Äù‚Äö exit code: 1\n",
      "        √¢‚Ä¢¬∞√¢‚Äù‚Ç¨> [21 lines of output]\n",
      "            + c:\\Users\\alvar\\Documents\\DataAvengers\\.venv\\Scripts\\python.exe C:\\Users\\alvar\\AppData\\Local\\Temp\\pip-install-7w4x7ltl\\numpy_feee14f7066249a6ae7a3e2bd40d8dcf\\vendored-meson\\meson\\meson.py setup C:\\Users\\alvar\\AppData\\Local\\Temp\\pip-install-7w4x7ltl\\numpy_feee14f7066249a6ae7a3e2bd40d8dcf C:\\Users\\alvar\\AppData\\Local\\Temp\\pip-install-7w4x7ltl\\numpy_feee14f7066249a6ae7a3e2bd40d8dcf\\.mesonpy-iam15vw6 -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --native-file=C:\\Users\\alvar\\AppData\\Local\\Temp\\pip-install-7w4x7ltl\\numpy_feee14f7066249a6ae7a3e2bd40d8dcf\\.mesonpy-iam15vw6\\meson-python-native-file.ini\n",
      "            The Meson build system\n",
      "            Version: 1.4.99\n",
      "            Source dir: C:\\Users\\alvar\\AppData\\Local\\Temp\\pip-install-7w4x7ltl\\numpy_feee14f7066249a6ae7a3e2bd40d8dcf\n",
      "            Build dir: C:\\Users\\alvar\\AppData\\Local\\Temp\\pip-install-7w4x7ltl\\numpy_feee14f7066249a6ae7a3e2bd40d8dcf\\.mesonpy-iam15vw6\n",
      "            Build type: native build\n",
      "            Project name: NumPy\n",
      "            Project version: 2.0.2\n",
      "            WARNING: Failed to activate VS environment: Could not find C:\\Program Files (x86)\\Microsoft Visual Studio\\Installer\\vswhere.exe\n",
      "      \n",
      "            ..\\meson.build:1:0: ERROR: Unknown compiler(s): [['icl'], ['cl'], ['cc'], ['gcc'], ['clang'], ['clang-cl'], ['pgcc']]\n",
      "            The following exception(s) were encountered:\n",
      "            Running `icl \"\"` gave \"[WinError 2] El sistema no puede encontrar el archivo especificado\"\n",
      "            Running `cl /?` gave \"[WinError 2] El sistema no puede encontrar el archivo especificado\"\n",
      "            Running `cc --version` gave \"[WinError 2] El sistema no puede encontrar el archivo especificado\"\n",
      "            Running `gcc --version` gave \"[WinError 2] El sistema no puede encontrar el archivo especificado\"\n",
      "            Running `clang --version` gave \"[WinError 2] El sistema no puede encontrar el archivo especificado\"\n",
      "            Running `clang-cl /?` gave \"[WinError 2] El sistema no puede encontrar el archivo especificado\"\n",
      "            Running `pgcc --version` gave \"[WinError 2] El sistema no puede encontrar el archivo especificado\"\n",
      "      \n",
      "            A full log can be found at C:\\Users\\alvar\\AppData\\Local\\Temp\\pip-install-7w4x7ltl\\numpy_feee14f7066249a6ae7a3e2bd40d8dcf\\.mesonpy-iam15vw6\\meson-logs\\meson-log.txt\n",
      "            [end of output]\n",
      "      \n",
      "        note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "      error: metadata-generation-failed\n",
      "      \n",
      "      √É‚Äî Encountered error while generating package metadata.\n",
      "      √¢‚Ä¢¬∞√¢‚Äù‚Ç¨> See above for output.\n",
      "      \n",
      "      note: This is an issue with the package mentioned above, not pip.\n",
      "      hint: See above for details.\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "√ó pip subprocess to install build dependencies did not run successfully.\n",
      "‚îÇ exit code: 1\n",
      "‚ï∞‚îÄ> See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install torch transformers beautifulsoup4 pdfplumber PyMuPDF langchain_core langchain_experimental langchain_community langchain_huggingface langchain_huggingface faiss-cpu pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9437b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import re\n",
    "import transformers\n",
    "import torch\n",
    "import bs4\n",
    "import pdfplumber\n",
    "import pymupdf\n",
    "import fitz\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Empty content on page\")\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.schema import Document\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_community.vectorstores.faiss import FAISS, DistanceStrategy\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline\n",
    "from langchain.document_loaders import (\n",
    "    PyMuPDFLoader,\n",
    "    TextLoader,\n",
    "    WebBaseLoader,\n",
    "    CSVLoader,\n",
    "    PDFPlumberLoader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd8f407",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_MODEL = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "EMBEDDINGS_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "# DOCS_DIR = os.path.join(DATA_DIR, 'ejemplo_alberto')\n",
    "DOCS_DIR = r\"Data\\Datasets\"\n",
    "BBDD_VECTORES_DIR = r\"Data\\BBDD\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9366eabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataBase():\n",
    "    \"\"\"\n",
    "        Class for store vectors\n",
    "\n",
    "        Params:\n",
    "        -------\n",
    "            docs_path: documents location in case of building the vectorstore\n",
    "            load_path: location of an existing vectorstore.\n",
    "            db_path: location of the new vectorstore in case docs_path != False\n",
    "            embedding_path: embeddings model location\n",
    "        Attributes:\n",
    "        -----------\n",
    "            embeddings: embeddings model used\n",
    "            vectorstore: vectorstore used\n",
    "        Methods:\n",
    "        --------\n",
    "            load_txt():\n",
    "                Create multiple Langchain.core.Document from a .txt file\n",
    "            load_csv():\n",
    "                Create multiple Langchain.core.Document from a .csv file\n",
    "            load_web():\n",
    "                Create multiple Langchain.core.Document from a url\n",
    "            load_tables():\n",
    "                Create multiple Langchain.core.Document from tables of a .pdf file\n",
    "            load_pdf():\n",
    "                Create multiple Langchain.core.Document from a .pdf file\n",
    "            load_documents():\n",
    "                Create a list of Langchain.core.Document from a directory\n",
    "            add_documents():\n",
    "                Add documents\n",
    "            search_documents():\n",
    "                Search documents given a query\n",
    "    \"\"\"\n",
    "    def __init__(self, docs_path=DOCS_DIR, load_path=False, db_path=BBDD_VECTORES_DIR, embedding_path=EMBEDDINGS_MODEL):\n",
    "        print(\"Loading Embeddings\")\n",
    "        self.embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=embedding_path,\n",
    "            model_kwargs={'trust_remote_code': True, 'device': 'cpu'},\n",
    "            encode_kwargs={'batch_size': 2}\n",
    "            )\n",
    "        if docs_path:\n",
    "            print(\"Loading Docs\")\n",
    "            docs = self.load_documents(docs_path)\n",
    "            print(f\"Cantidad de documentos: {len(docs)}\")\n",
    "            print(\"Splitting Docs\")\n",
    "            # splits = SemanticChunker(self.embeddings).split_documents(docs)\n",
    "            print(\"Creating Vectors to FAISS DB\")\n",
    "            self.vectorstore = FAISS.from_documents(docs, self.embeddings, normalize_L2=True, distance_strategy=DistanceStrategy.MAX_INNER_PRODUCT)\n",
    "            print(\"Saving Vectors to FAISS DB\")\n",
    "            self.vectorstore.save_local(db_path)\n",
    "        elif load_path:\n",
    "            self.vectorstore = FAISS.load_local(load_path, self.embeddings, allow_dangerous_deserialization=True)\n",
    "        else:\n",
    "            raise ValueError(\"docs_path or load_path must be different of False\")\n",
    "\n",
    "    def load_txt(self, file):\n",
    "        loader = TextLoader(file, encoding='utf-8')\n",
    "        docs = loader.load()\n",
    "        return docs\n",
    "\n",
    "    def load_csv(self, file):\n",
    "        df = pd.read_csv(file, encoding='utf-8')\n",
    "        docs = []\n",
    "        for idx, row in df.iterrows():\n",
    "            row_text = row.to_json(force_ascii=False)\n",
    "            docs.append(Document(page_content=row_text, metadata={\"source\": file, \"row\": idx}))\n",
    "        return docs\n",
    "\n",
    "    def load_web(self, url):\n",
    "        loader = WebBaseLoader(\n",
    "            web_paths=(url),\n",
    "            bs_kwargs=dict(\n",
    "                parse_only=bs4.SoupStrainer(\n",
    "                    class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "                )\n",
    "            ),\n",
    "        )\n",
    "        docs = loader.load()\n",
    "        return docs\n",
    "\n",
    "    def load_tables(self, file):\n",
    "        docs = []\n",
    "        with pdfplumber.open(file) as pdf:\n",
    "            for page_number, page in enumerate(pdf.pages):\n",
    "                tables = page.extract_tables()\n",
    "                for table in tables:\n",
    "                    df = pd.DataFrame(table[1:], columns=table[0])\n",
    "                    df_unique = df.loc[:, ~df.columns.duplicated()]\n",
    "                    json_text = df_unique.to_json(orient='records', lines=False)\n",
    "                    json_text = json_text.replace('\\\\n', '\\n')\n",
    "                    json_text = re.sub(r'\\\\/', '/', json_text)\n",
    "                    json_text = json_text.encode('utf-8').decode('unicode_escape')\n",
    "                    doc = Document(\n",
    "                        page_content=json_text,\n",
    "                        metadata={\"source\": file, \"type\": \"table\"}\n",
    "                    )\n",
    "                    docs.append(doc)\n",
    "        return docs\n",
    "\n",
    "    def load_pdf(self, file):\n",
    "        docs = []\n",
    "\n",
    "        def process_page(page_num):\n",
    "            try:\n",
    "                with pdfplumber.open(file) as pdf:\n",
    "                    page = pdf.pages[page_num]\n",
    "                    text = page.extract_text()\n",
    "                    if text:\n",
    "                        return Document(page_content=text, metadata={\"source\": file, \"page\": page_num})\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing page {page_num} in {file}: {e}\")\n",
    "            return None\n",
    "\n",
    "        with pdfplumber.open(file) as pdf:\n",
    "            num_pages = len(pdf.pages)\n",
    "\n",
    "        max_workers = min(2, num_pages)\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = [executor.submit(process_page, page_num) for page_num in range(num_pages)]\n",
    "            for future in as_completed(futures):\n",
    "                doc_result = future.result()\n",
    "                if doc_result:\n",
    "                    docs.append(doc_result)\n",
    "\n",
    "        return docs\n",
    "\n",
    "    def load_documents(self, path_directory):\n",
    "        docs = []\n",
    "        for root, _, files in os.walk(path_directory):  # os.walk recorre todas las subcarpetas\n",
    "            for file in files:\n",
    "                print(file)\n",
    "                file_path = os.path.join(root, file)\n",
    "                if file.endswith('.txt'):\n",
    "                    docs.extend(self.load_txt(file_path))\n",
    "                elif file.endswith('.csv'):\n",
    "                    docs.extend(self.load_csv(file_path))\n",
    "                elif file.endswith('.pdf'):\n",
    "                    docs.extend(self.load_pdf(file_path))\n",
    "        return docs\n",
    "\n",
    "    def add_documents(self, path_file):\n",
    "        if path_file.endswith('.txt'):\n",
    "            docs = self.load_txt(path_file)\n",
    "        elif path_file.endswith('.csv'):\n",
    "            docs = self.load_csv(path_file)\n",
    "        elif path_file.endswith('.pdf'):\n",
    "            docs = self.load_pdf(path_file)\n",
    "        self.vectorstore.add_documents(docs)\n",
    "\n",
    "    def search_documents(self, query, num=4, score=False):\n",
    "        if score:\n",
    "            return self.vectorstore.similarity_search(query, k=num)\n",
    "        else:\n",
    "            return self.vectorstore.similarity_search(query, k=num, score_threshold=score)\n",
    "\n",
    "class RagChain():\n",
    "    \"\"\"\n",
    "        Class for making a chain with questions\n",
    "\n",
    "        Params:\n",
    "        -------\n",
    "            query: question made\n",
    "            db: DataBase object\n",
    "            model: HuggingFacePipeline object\n",
    "        Attributes:\n",
    "        -----------\n",
    "            query: list of questions\n",
    "            context: list of contexts used to answer the query\n",
    "            prompt: prompt used in the last query\n",
    "            answer: list of answers obtained from the queries\n",
    "        Methods:\n",
    "        --------\n",
    "            format_docs:\n",
    "                Return a formated string to make the prompt\n",
    "            question:\n",
    "                Make another question with context of the previous questions\n",
    "    \"\"\"\n",
    "    def __init__(self, query, db, model):\n",
    "        self.query = []\n",
    "        self.query.append(query)\n",
    "        self.context = []\n",
    "        self.context.append(self.format_docs(db.search_documents(self.query[0])))\n",
    "        self.prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "            Eres un asistente para preguntas y respuestas. Responde a la siguiente pregunta usando solo la informaci√≥n proporcionada en el contexto. Si la respuesta no est√° en el contexto, di 'No lo s√©'.<|eot_id|>\n",
    "            <|start_header_id|>user<|end_header_id|>\n",
    "            Contexto: {self.context[0]}\n",
    "\n",
    "            Pregunta: {self.query[0]}<|eot_id|>\n",
    "            <|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "        self.answer = []\n",
    "        self.answer.append(StrOutputParser().parse(model.invoke(self.prompt)).split(\"<|end_header_id|>\")[-1].replace(\"\\n\", \"\"))\n",
    "\n",
    "    def format_docs(self, docs):\n",
    "        return \"\\n\\n--\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    def question(self, query, db, model):\n",
    "        aux_queries = self.query[-3:]\n",
    "        aux_answer = self.answer[-3:]\n",
    "        prompt_answer = \"\"\n",
    "        for q in aux_queries:\n",
    "            ans = aux_answer[aux_queries.index(q)]\n",
    "            prompt_answer = prompt_answer + f\"\\n\\n <|start_header_id|>user<|end_header_id|>Pregunta: {q}<|eot_id|> \\n\\n <|start_header_id|>assistant<|end_header_id|>Respuesta: {ans}<|eot_id|>\"\n",
    "        self.query.append(query)\n",
    "        self.context.append(self.format_docs(db.search_documents(query)))\n",
    "        self.prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "            Act√∫a como un asistente de generaci√≥n de informes. Crea un resumen ejecutivo del siguiente contenido, usando t√≠tulos, subt√≠tulos, y listas con vi√±etas. El resumen debe ser claro, bien estructurado, y f√°cil de leer. Formatea el texto utilizando Markdown para que sea f√°cil de renderizar en un documento final.<|eot_id|>\n",
    "            {prompt_answer}\n",
    "            <|start_header_id|>user<|end_header_id|>\n",
    "            Contexto: {self.context[-1]}\n",
    "\n",
    "            Pregunta: {self.query[-1]}<|eot_id|>\n",
    "            <|start_header_id|>assistant<|end_header_id|>\n",
    "            \"\"\"\n",
    "        self.answer.append(StrOutputParser().parse(model.invoke(self.prompt)).split(\"<|end_header_id|>\")[-1].replace(\"\\n\", \"\"))\n",
    "\n",
    "def read_excel(path_file):\n",
    "    list_query = []\n",
    "    df = pd.read_excel(path_file)\n",
    "    for index, rows in df.iterrows():\n",
    "       list_query.append(rows[0])\n",
    "    return list_query\n",
    "\n",
    "def write_answer(chains, path_file):\n",
    "    df = pd.DataFrame()\n",
    "    for chain in chains:\n",
    "        aux = pd.DataFrame({'Pregunta': chain.query, 'Respuesta': chain.answer, 'Contexto': chain.context})\n",
    "        df = pd.concat([df,aux],ignore_index=True).drop_duplicates()\n",
    "    df.to_excel(path_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ebe609d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Database\n",
      "Loading Embeddings\n",
      "Loading Docs\n",
      "DataAnalyst.csv\n",
      "Cantidad de documentos: 2253\n",
      "Splitting Docs\n",
      "Creating Vectors to FAISS DB\n",
      "Saving Vectors to FAISS DB\n",
      "Database loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alvar\\Documents\\DataAvengers\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:233: UserWarning: Normalizing L2 is not applicable for metric type: DistanceStrategy.MAX_INNER_PRODUCT\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(\"Start Database\")\n",
    "db = DataBase(DOCS_DIR, load_path=BBDD_VECTORES_DIR)\n",
    "print(\"Database loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5e113611",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"I am a data analyst with 3 years of experience looking for a full-time position in a dynamic company. I have a strong background in SQL, Python, and Excel, and have worked extensively with Power BI and Tableau to create dashboards and business reports. I‚Äôm particularly interested in roles that offer remote work or are based in New York or San Francisco. I value collaborative environments where I can contribute to data-driven decision making and continue learning. Ideally, I‚Äôm looking for a salary around $100,000 and opportunities for career growth.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "53c14853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Pregunta: I am a data analyst with 3 years of experience looking for a full-time position in a dynamic company. I have a strong background in SQL, Python, and Excel, and have worked extensively with Power BI and Tableau to create dashboards and business reports. I‚Äôm particularly interested in roles that offer remote work or are based in New York or San Francisco. I value collaborative environments where I can contribute to data-driven decision making and continue learning. Ideally, I‚Äôm looking for a salary around $100,000 and opportunities for career growth.\n",
      "\n",
      "--- Resultado 1 ---\n",
      "{\"Unnamed: 0\":1396,\"Job Title\":\"Direct Client Requirement - Data Reporting Analyst\",\"Salary Estimate\":\"$41K-$86K (Glassdoor est.)\",\"Job Description\":\"Our direct client which is a leading telecommunications and media company is looking for a Data Reporting Analyst in Dallas, TX. Please submit local candidates only. It's long term contract role, please share resume asap with Paulrealsoftinc.com. Local candidates only Role Data Reporting Analyst Location 211 Akard St Dallas, TX 75202 Duration 1 year+ contract Job description We are looking for a data analyst to join an exciting opportunity within our client. This team is working on cutting edge Robotics Process Automation tools to help become more efficient and prepped for the future of AI in business. This role will start off as in an analyst role, in which you will be helping out various teams with data analysis and planning. The opportunity for growth is paramount in this exciting space. Responsibilities 5-8+ years of data analytics development experience (with a primary focus on Microsoft). 2+ years of Microsoft Power BI experience (Power BI) or 3+ years of experience Tableau or QlikView httpswww.qlik.comusproductsqlikview or Microstratey Strong concepts of fueling business process improvement and decision making through effective use of business intelligence. Expert understanding of SQL, or similar database programming technologies. Gather requirements, document and design in order to implement and develop new reporting and analyticals solutions. Design implement and develop data visualizations. Meet with business stakeholders (Directors to C-level) to clarify and document reporting requirements as well as present findings. Design data architecture and engineering structures necessary to support our BI initiatives. Data integration management between core business applications and data lakes, data marts, etc. Able to work independently to implement a solution with minimal guidance. Communicate effectively with both business, technical stakeholders as well as executive leadership. Basic Qualifications Required Qualifications Bachelor's degree or equivalent experience 5 years of experience developing Power BI solutions and reporting OR Tableau OR QlikView or Microstratey Must have SSIS and SSRS and ETL experience Advanced MS Excel and MS PowerPoint 5 years of experience in Data Visualization and Analysis Experience working with dataset ingestion, data model creation, reports, dashboards, KPIs, Power BI Pro features, workspaces, security, etc.\",\"Rating\":-1.0,\"Company Name\":\"Real Soft, Inc \\/ Diversity Direct\",\"Location\":\"Dallas, TX\",\"Headquarters\":\"-1\",\"Size\":\"-1\",\"Founded\":-1,\"Type of ownership\":\"-1\",\"Industry\":\"-1\",\"Sector\":\"-1\",\"Revenue\":\"-1\",\"Competitors\":\"-1\",\"Easy Apply\":\"-1\"}\n",
      "\n",
      "--- Resultado 2 ---\n",
      "{\"Unnamed: 0\":2168,\"Job Title\":\"Data Support Analyst\",\"Salary Estimate\":\"$57K-$67K (Glassdoor est.)\",\"Job Description\":\"Greenwood Village, Colorado\\nSkills : sql\\nDescription :\\nRequired Qualifications:\\n3-5+ years of data analyst experience\\nHive, Kibana, and\\/or Splunk\\nStrong investigative skills and highly analytical mindset\\nTroubleshooting API calls\\nSQL\\nBonus Skills:\\nExperience with connected devices such as Xbox, Roku, PlayStation, etc. (this doesn‚Äôt have to be professional, this could be in spare time)\\nInterest in cord-cutting technologies\\nCable\\/telecom background\\nResponsibilities:\\nWork on a data platform team responsible for conducting client investigations through data in order to solve customer issues\\nTriage customer feedback from applications\\nTest and certify products for release to application stores\\nWork on customer issues and outages\\nDocument product\\/testing changes\\nWork in a cross-functional environment with several internal stakeholderssql\",\"Rating\":4.1,\"Company Name\":\"Collabera\\n4.1\",\"Location\":\"Greenwood Village, Arapahoe, CO\",\"Headquarters\":\"Basking Ridge, NJ\",\"Size\":\"10000+ employees\",\"Founded\":1991,\"Type of ownership\":\"Company - Private\",\"Industry\":\"IT Services\",\"Sector\":\"Information Technology\",\"Revenue\":\"$500 million to $1 billion (USD)\",\"Competitors\":\"Kforce, Insight Global, Volt Consulting Group\",\"Easy Apply\":\"-1\"}\n",
      "\n",
      "--- Resultado 3 ---\n",
      "{\"Unnamed: 0\":2138,\"Job Title\":\"Healthcare Business and Data Analyst\",\"Salary Estimate\":\"$55K-$101K (Glassdoor est.)\",\"Job Description\":\"Job Description\\nIntro: We have been retained by our fast-growing Downtown Seattle client (a hot web SAAS provider of advanced healthcare services). Our client needs a Strong Business Data Analyst with great experience in the healthcare industry\\n\\nRequired Experience:\\nStrong Business and Data Analyst (several years experience leveraging data and analytics to solve healthcare oriented business problems)\\nStrong SQL and DBMS experience\\nSome Python experience\\nExperience with tools such as Tableau or Hadoop\\nHealthcare and HIPAA experience strongly preferred\\ndegree in - technical field such as Data Science, Computer Science, Information Science, Mathematics, Statistics,\\nA startup mindset (fast moving, teaming).\\nLocation and Compensation: Staff role with a fast-growing company known for treating its employees exceptionally well. Our client is in downtown Seattle and is high accessible.. Excellent compensation and great benefits. Candidates should be local Settle or willing to relocate quickly and at low\\/reasonable cost for the opportunity.\\n\\nAbout TechTalent: TechTalent specializes in recruiting for Media and Web Services \\/ Mobile Software clients. Reach out to Jon - I am a unique recruiter: heavy tech background (CS and MSEE degree from Georgia Tech) with 15 years of technical experience as a software engineer, and over 8 as a senior technical manager (VP level) for companies that deployed digital media systems. I own and run a truly technical recruiting practice. I can help you get the best job and a great offer.\",\"Rating\":-1.0,\"Company Name\":\"Jon French \\/ TechTalent Services\",\"Location\":\"Seattle, WA\",\"Headquarters\":\"-1\",\"Size\":\"-1\",\"Founded\":-1,\"Type of ownership\":\"-1\",\"Industry\":\"-1\",\"Sector\":\"-1\",\"Revenue\":\"-1\",\"Competitors\":\"-1\",\"Easy Apply\":\"-1\"}\n",
      "\n",
      "--- Resultado 4 ---\n",
      "{\"Unnamed: 0\":533,\"Job Title\":\"Data Analyst\",\"Salary Estimate\":\"$55K-$103K (Glassdoor est.)\",\"Job Description\":\"DatamanUSA LLC has an exciting opportunity for a Data Analyst√Çto work onsite with one of our direct clients in Los Angeles, CA. If you are not available or not interested then we love referrals! Please refer us to your friends, family, and colleagues for this opportunity. DatamanUSA gives referral bonuses (up to $500) if they get selected and perform well for our clients. Position Title: Data Analyst Duration: 1+ Month (with possible extension) Location: Los Angeles, California Required Skills and Experience:\\n4+ years of work experience with data analysis, statistical analysis, and data mining in business administration, public administration, IT, or other related industry.\\n4+ years of work experience gathering data requirements (including functional, user, system requirements); creating application wireframes, developing process flow diagrams, writing quality technical and user guide documentation; perform heavy research; and any other analysis tasks necessary for software implementation projects.\\n4+ years of work experience using SQL databases and database query languages.\\nExtensive experience in data management, data analysis, managing large data sets, data scrubbing, and identifying and resolving data, code or scripting issues.\\nIntermediate to advanced knowledge of Microsoft Word, Excel, Project, and PowerPoint is required.\\nIntermediate to advanced knowledge of Microsoft Access and Outlook are strongly desired.\",\"Rating\":3.4,\"Company Name\":\"DatamanUSA\\n3.4\",\"Location\":\"Los Angeles, CA\",\"Headquarters\":\"Centennial, CO\",\"Size\":\"51 to 200 employees\",\"Founded\":-1,\"Type of ownership\":\"Company - Private\",\"Industry\":\"IT Services\",\"Sector\":\"Information Technology\",\"Revenue\":\"$5 to $10 million (USD)\",\"Competitors\":\"-1\",\"Easy Apply\":\"-1\"}\n",
      "\n",
      "--- Resultado 5 ---\n",
      "{\"Unnamed: 0\":54,\"Job Title\":\"Data Scientist \\/ Big Data Analytics Analyst\",\"Salary Estimate\":\"$46K-$87K (Glassdoor est.)\",\"Job Description\":\"TSR is a premier National U.S. Staffing company with over 50 years of staffing excellence.Our client, a leading financial company is hiring a Big Data Analyst on a contract basis.Work Location:Weehawken, NJMust have:Data Science \\/ Big Data ExperienceRecent hands on experience w\\/ Alteryx and PythonUndergraduate degree in Computer Science or Data ScienceExcellent communication skillsHighly preferred: DataRobot experienceYour roleDo you enjoy consulting and working on digital transformation projects? Are you looking for a unique opportunity to contribute to the firm's accelerating digital transformation? Are you passionate about new technologies like: Artificial Intelligence, Machine Learning, Cognitive Automation, Advanced Analytics, Blockchain.Do you have hands on experience w\\/ Alteryx and Python?Your teamYou will be working in the brand new Digital Practice unit based in Weehawken. We are a dynamic and diverse team of transformation professionals with focus on emerging technology transformation. This unit is part of the broader newly created Group Internal Consulting department.Our practice teams help our firm's business functions implement their digital strategy and accomplish their ambitions in the areas of emerging technologies like Artificial Intelligence (AI), Advanced Data Analytics, Cognitive Automation and Blockchain. The Digital Practice is at the intersection of business, technology and innovative thinking. We accompany functions across all businesses to build a sustainable transformation; combining the power of emerging technologies with a profound understanding of the firm's businesses. The team works in in close collaboration with the different functions, business divisions and external parties to achieve its goals.Your expertise* At least 3 years of hands on consulting experience with a prominent or boutique consulting firm, with recent projects focused on digital transformation and new technologies.* Knowledge and hands on working experience in one or more of the following areas: Artificial Intelligence, Advanced Analytics, Cognitive Automation, Blockchain.* Recent hands on experience w\\/ Alteryx and Python* Strong data analytical background, attention to detail along with structured organization and planning skills.* Highly driven and motivated with a can-do attitude and eager to learn and to solve complex problems.* A strong written and verbal communicator, able to adapt and network across a global organization to work well with the team and to present well to stakeholders.A plus:* Experience with any of these tools is a plus: DataIKU, Tableau, DataRobot, Workfusion, re:infer, Teneo, AmeliaEducation:* Undergraduate degree: Computer Science, Data Science from a top tier universityPlease contact me directly if you are interested in learning more about this opportunity or to discuss our referral program if you know anyone that may be interested.\",\"Rating\":3.6,\"Company Name\":\"TSR, Inc.\\n3.6\",\"Location\":\"Weehawken, NJ\",\"Headquarters\":\"Hauppauge, NY\",\"Size\":\"201 to 500 employees\",\"Founded\":1969,\"Type of ownership\":\"Company - Public\",\"Industry\":\"Staffing & Outsourcing\",\"Sector\":\"Business Services\",\"Revenue\":\"$50 to $100 million (USD)\",\"Competitors\":\"-1\",\"Easy Apply\":\"-1\"}\n"
     ]
    }
   ],
   "source": [
    "for query in queries:\n",
    "    print(f\"\\nüîç Pregunta: {query}\")\n",
    "    results = db.search_documents(query, num=5)  # Puedes cambiar el n√∫mero de resultados\n",
    "    for i, doc in enumerate(results, 1):\n",
    "        print(f\"\\n--- Resultado {i} ---\")\n",
    "        print(doc.page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5b866500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recupera todos los documentos almacenados\n",
    "docs = db.vectorstore.docstore._dict.values()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
